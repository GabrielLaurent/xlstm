model:
  type: LSTM
  input_size: 10
  hidden_size: 20
  num_layers: 1
  output_size: 10

data:
  vocab_size: 10
  sequence_length: 20

training:
  epochs: 10
  batch_size: 32
  learning_rate: 0.01